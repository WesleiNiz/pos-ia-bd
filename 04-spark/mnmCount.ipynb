{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnmCount.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Contando MMs\n",
        "\n",
        "Uma cientista de dados que adora assar biscoitos com M&Ms e recompensar seus alunos nos estados dos EUA, onde frequentemente ministra cursos de aprendizado de máquina e ciência de dados com lotes desses biscoitos. Mas ela quer garantir que ela obtenha as cores certas de M&Ms nos biscoitos para estudantes em diferentes estados.\n",
        "\n",
        "Neste exemplo vamos apresentar um programa Spark que lê um arquivo com mais de 100.000 entradas (onde cada linha ou linha tem um `<state, mnm_color, count>`) e calcula as contagens para cada cor e estado. \n",
        "\n",
        "\n",
        "<img src=\"https://st2.depositphotos.com/2035563/6378/i/600/depositphotos_63782833-stock-photo-candy-dots-background.jpg\" class=\"bg-primary mb-1\" width=\"300px\">"
      ],
      "metadata": {
        "id": "N7xl7pt7wD70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conectando o Drive ao Colab\n",
        "\n",
        "A primeira coisa que você quer fazer quando está trabalhando no Colab é montar seu Google Drive. Isso permitirá que você acesse qualquer diretório em seu Drive dentro do notebook Colab."
      ],
      "metadata": {
        "id": "TVhZCcPdqm52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7sOGygL7q3Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PySpark no Google Colab\n",
        "\n",
        "PySpark é a interface alto nível que permite você conseguir acessar e usar o Spark por meio da linguagem Python. Usando o PySpark, você consegue escrever todo o seu código usando apenas o estilo Python de escrever código.\n",
        "\n",
        "Já o Google Colab é uma ferramenta incrível, poderosa e gratuita – com suporte de GPU inclusive. Uma vez que roda 100% na nuvem, você não tem a necessidade de instalar qualquer coisa na sua própria máquina.\n",
        "\n",
        "No entanto, apesar da maioria das bibliotecas de Data Science estarem previamente instaladas no Colab, o mesmo não acontece com o PySpark. Para conseguir usar o PySpark é necessário alguns passos intermediários, que não são triviais para aqueles que estão começando.\n",
        "\n",
        "Dessa maneira, preparei um tutorial simples e direto ensinando a instalar as dependências e a biblioteca.\n",
        "\n",
        "## Instalando o PySpark no Google Colab\n",
        "\n",
        "Instalar o PySpark não é um processo direto como de praxe em Python. Não basta usar um pip install apenas. Na verdade, antes de tudo é necessário instalar dependências como o Java 8, Apache Spark 2.3.2 junto com o Hadoop 2.7."
      ],
      "metadata": {
        "id": "CWCwj_pRtBvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instalar as dependências\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "euUUXiXwtPXb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A próxima etapa é configurar as variáveis de ambiente, pois isso habilita o ambiente do Colab a identificar corretamente onde as dependências estão rodando.\n",
        "\n",
        "Para conseguir “manipular” o terminal e interagir como ele, você pode usar a biblioteca os."
      ],
      "metadata": {
        "id": "Q706reA7ti4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configurar as variáveis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "# tornar o pyspark \"importável\"\n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')"
      ],
      "metadata": {
        "id": "HX-mOya7tpwB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com tudo pronto, vamos rodar uma sessão local para testar se a instalação funcionou corretamente."
      ],
      "metadata": {
        "id": "JRDB1T3ut0Io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# iniciar uma sessão local e importar dados do Airbnb\n",
        "from pyspark.sql import SparkSession\n",
        "sc = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "\n",
        "# carregar dados do Airbnb\n",
        "df_spark = sc.read.csv(\"/content/drive/MyDrive/Ensino/IFG/2022/01/Big_Data/pos-ia-bd/04-spark/data/mnm_dataset.csv\", inferSchema=True, header=True)\n",
        "\n",
        "# ver algumas informações sobre os tipos de dados de cada coluna\n",
        "df_spark.printSchema()"
      ],
      "metadata": {
        "id": "y7afh0SZt8V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementando a aplicação\n",
        "\n",
        "A primeira coisa a fazer é criar a aplicação e a sessão do Spark."
      ],
      "metadata": {
        "id": "eOYHhjXZyJdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = (SparkSession\n",
        "         .builder\n",
        "         .appName(\"PythonMnMCount\")\n",
        "         .getOrCreate())"
      ],
      "metadata": {
        "id": "LAs8-NjWyayd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obter o nome do arquivo do conjunto de dados M&M"
      ],
      "metadata": {
        "id": "ZBkmGz1GysLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnm_file = \"/content/drive/MyDrive/Ensino/IFG/2022/01/Big_Data/pos-ia-bd/04-spark/data/mnm_dataset.csv\""
      ],
      "metadata": {
        "id": "qMj4UK1vyzZp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ler o arquivo em um Spark DataFrame usando o formato CSV inferindo o esquema e especificando que o arquivo contém um cabeçalho, que fornece nomes dos campos."
      ],
      "metadata": {
        "id": "lzCKRj9ty-RW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnm_df = (spark.read.format(\"csv\")\n",
        "          .option(\"header\", \"true\")\n",
        "          .option(\"inferSchema\", \"true\")\n",
        "          .load(mnm_file))\n",
        "mnm_df.show(n=5, truncate=False)"
      ],
      "metadata": {
        "id": "yDRTrhzQzRbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos as APIs de alto nível do DataFrame. Observe que não usamos RDDs. Como algumas das funções do Spark retornam o mesmo objeto, podemos encadear chamadas de função.\n",
        "1. Selecione no DataFrame os campos \"Estado\", \"Cor\" e \"Contagem\"\n",
        "2. Como queremos agrupar cada estado e sua contagem de cores M&M, usamos `groupBy()`\n",
        "3. Contagens agregadas de todas as cores e estado e cor `groupBy()`\n",
        "4. `orderBy()` em ordem decrescente"
      ],
      "metadata": {
        "id": "bVzdiEwlzy0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_mnm_df = (mnm_df.select(\"State\", \"Color\", \"Count\")\n",
        "                    .groupBy(\"State\", \"Color\")\n",
        "                    .sum(\"Count\")\n",
        "                    .orderBy(\"sum(Count)\", ascending=False))"
      ],
      "metadata": {
        "id": "Cq5EMUBa0Hwn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostre as agregações resultantes para todos os estados e cores; uma contagem total de cada cor por estado.\n",
        "\n",
        "Observe que `show()` é uma ação que acionará a execução da consulta acima."
      ],
      "metadata": {
        "id": "_L8TrXfO0R9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_mnm_df.show(n=60, truncate=False)\n",
        "print(\"Total Rows = %d\" % (count_mnm_df.count()))"
      ],
      "metadata": {
        "id": "nvL060f71z-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enquanto o código acima é agregado e contado para todos os estados, e se quisermos apenas ver os dados de um único estado, por exemplo, CA?\n",
        "1. Selecione de todas as linhas no DataFrame\n",
        "2. Filtre apenas o estado da CA\n",
        "3. `groupBy()` Estado e Cor como fizemos acima\n",
        "4. Agregue as contagens para cada cor\n",
        "5. `orderBy()` em ordem decrescente"
      ],
      "metadata": {
        "id": "E_1YFJR-2RVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ca_count_mnm_df = (mnm_df.select(\"*\")\n",
        "                       .where(mnm_df.State == 'CA')\n",
        "                       .groupBy(\"State\", \"Color\")\n",
        "                       .sum(\"Count\")\n",
        "                       .orderBy(\"sum(Count)\", ascending=False))"
      ],
      "metadata": {
        "id": "fPa0h20x2gjL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostre a agregação resultante para a Califórnia.\n",
        "\n",
        "Como acima, `show()` é uma ação que acionará a execução de toda a computação."
      ],
      "metadata": {
        "id": "ANjMmXMc2sFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ca_count_mnm_df.show(n=10, truncate=False)"
      ],
      "metadata": {
        "id": "o5GLK_7A2yfx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}